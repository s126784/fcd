{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Football Player Data Analysis\"\n",
        "subtitle: \"Part 3\"\n",
        "author:\n",
        "  - name: \"Oleksandr Solovei\"\n",
        "    affiliations:\n",
        "        - 126784\n",
        "        - https://github.com/s126784/fcd/\n",
        "format:\n",
        "  revealjs:\n",
        "    slide-number: true\n",
        "    show-slide-number: all\n",
        "    chalkboard:\n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    logo: static/ua.png\n",
        "    css: static/mystyle.css\n",
        "    theme: default\n",
        "    transition: slide\n",
        "    # width: 1280\n",
        "    # height: 720\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Project Evolution\n",
        "\n",
        "#### Previous Parts\n",
        "\n",
        " - Part 1: Data Collection & Initial Analysis\n",
        " - Part 2: Text Analysis & Historical Data\n",
        "\n",
        "::: {.fragment}\n",
        "#### Part 3 Goals\n",
        "\n",
        " - Advanced text processing & sentiment analysis\n",
        " - Time series prediction for market values\n",
        " - Player clustering and network visualization\n",
        " - Market value trend prediction\n",
        ":::\n",
        "\n",
        "## Dataset{.scrollable}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import urllib.parse\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import spacy\n",
        "\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from unicodedata import normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "years = [2014, 2015, 2016, 2017]\n",
        "data = list(map(lambda year: pd.read_csv(f'data/portugal_{year}_plus.csv'), years))\n",
        "data[0].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphical Representation (Matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_market_value_timeline(data_list, years):\n",
        "    # Create a DataFrame for timeline plotting\n",
        "    timeline_data = []\n",
        "\n",
        "    for year, df in zip(years, data_list):\n",
        "        year_data = df[['Name', 'Market value']].copy()\n",
        "        year_data['Year'] = year\n",
        "        year_data['Market value'] = year_data['Market value'].apply(lambda x: float(str(x).replace(',', '')))\n",
        "        year_data['Market value'] = year_data['Market value'] / 1000000\n",
        "        timeline_data.append(year_data)\n",
        "\n",
        "    timeline_df = pd.concat(timeline_data)\n",
        "\n",
        "    plt.figure(figsize=(16, 7))\n",
        "\n",
        "    # Create the line plot\n",
        "    sns.lineplot(data=timeline_df,\n",
        "                x='Year',\n",
        "                y='Market value',\n",
        "                hue='Name',\n",
        "                marker='o',\n",
        "                markersize=8)\n",
        "\n",
        "    # Set integer ticks on x-axis\n",
        "    plt.xticks(years)\n",
        "\n",
        "    plt.title('Player Market Values Over Time (2014-2017)', pad=20)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Market Value (Million €)')\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.ylim(bottom=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt\n",
        "\n",
        "# Create and display the timeline\n",
        "plt = create_market_value_timeline(data, years)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphical Representation (NetworkX)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged_data = pd.concat(data)\n",
        "merged_data = merged_data.groupby('Name').agg({'Market value': 'sum', 'Position': 'first', 'Player': 'first'}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_position_network(df):\n",
        "    # Create a new graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes (players)\n",
        "    for _, player in df.iterrows():\n",
        "        # Convert market value to float, removing commas\n",
        "        market_value = float(str(player['Market value']).replace(',', ''))\n",
        "        G.add_node(player['Name'],\n",
        "                  position=player['Position'],\n",
        "                  market_value=market_value)\n",
        "\n",
        "    # Connect players with same position\n",
        "    players = list(G.nodes(data=True))\n",
        "    for i in range(len(players)):\n",
        "        for j in range(i + 1, len(players)):\n",
        "            player1, data1 = players[i]\n",
        "            player2, data2 = players[j]\n",
        "            if data1['position'] == data2['position']:\n",
        "                G.add_edge(player1, player2)\n",
        "\n",
        "    # Create the visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Calculate node sizes based on market value (scaled for visibility)\n",
        "    node_sizes = [G.nodes[player]['market_value']/1000000 * 50 for player in G.nodes()]\n",
        "\n",
        "    # Create a color map based on positions\n",
        "    unique_positions = list(set(nx.get_node_attributes(G, 'position').values()))\n",
        "    color_map = {pos: plt.cm.Set3(i/len(unique_positions)) for i, pos in enumerate(unique_positions)}\n",
        "    node_colors = [color_map[G.nodes[player]['position']] for player in G.nodes()]\n",
        "\n",
        "    # Set up the layout\n",
        "    pos = nx.spring_layout(G, k=1, iterations=50)\n",
        "\n",
        "    # Draw the network\n",
        "    nx.draw_networkx_nodes(G, pos,\n",
        "                          node_size=node_sizes,\n",
        "                          node_color=node_colors,\n",
        "                          alpha=0.7)\n",
        "    nx.draw_networkx_edges(G, pos,\n",
        "                          edge_color='gray',\n",
        "                          alpha=0.3)\n",
        "\n",
        "    # Add labels\n",
        "    labels = {player: f\"{player}\\n{G.nodes[player]['position']}\\n{int(G.nodes[player]['market_value']/1000000)}M€\"\n",
        "              for player in G.nodes()}\n",
        "\n",
        "    nx.draw_networkx_labels(G, pos,\n",
        "                           labels=labels,\n",
        "                           font_size=8,\n",
        "                           font_weight='bold')\n",
        "\n",
        "    # Add legend for positions\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                 markerfacecolor=color, label=pos,\n",
        "                                 markersize=10)\n",
        "                      for pos, color in color_map.items()]\n",
        "    plt.legend(handles=legend_elements, title='Positions',\n",
        "              loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    plt.title('Portuguese Players (2014-2017)', pad=20)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return plt\n",
        "\n",
        "# Create and display the network using merged data, summing up market values of players with multiple entries\n",
        "plt = create_position_network(merged_data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "content_list = []\n",
        "\n",
        "for year, df in zip(years, data):\n",
        "    for i, row in df.iterrows():\n",
        "        fname = urllib.parse.quote_plus(row['Player'])\n",
        "        if (os.path.isfile(f'snapshots/{year}_{fname}.json')):\n",
        "                with open(f'snapshots/{year}_{fname}.json','r') as f:\n",
        "                    snapshot = f.read()\n",
        "                    arquive_data = json.loads(snapshot)\n",
        "\n",
        "                    # extract snippets from response_items and add them as a new column\n",
        "                    # to the dataframe\n",
        "                    for item in arquive_data['response_items']:\n",
        "                        snippet_data = {\n",
        "                            'Player': row['Player'],\n",
        "                            'title': item.get('title', ''),\n",
        "                            'url': item.get('originalURL', ''),\n",
        "                            'snippet': item.get('snippet', '')\n",
        "                        }\n",
        "                        content_list.append(snippet_data)\n",
        "\n",
        "        else:\n",
        "            print(f\"Snapshot not found for {row['Player']}\")\n",
        "            continue\n",
        "\n",
        "content_df = pd.DataFrame(content_list)\n",
        "\n",
        "# add preloaded HTML content\n",
        "\n",
        "html_content = pd.read_csv('data/url_content_extracted.csv')\n",
        "\n",
        "#join the column `extracted_text` to the content_df merging by url\n",
        "content_df = content_df.merge(html_content[['url', 'extracted_text']], on='url', how='left')\n",
        "\n",
        "# clean rows with missing content\n",
        "content_df = content_df.dropna(subset=['extracted_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def advanced_tokenization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    # Remove non-alphabetic and stopwords\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens\n",
        "             if t.isalpha() and t not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply advanced tokenization to content_df\n",
        "content_df['tokens'] = content_df['extracted_text'].apply(advanced_tokenization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# group tokens by player (merge arrays of strings)\n",
        "tokens_by_player = content_df.groupby('Player')['tokens'].sum()\n",
        "\n",
        "# create a word cloud for each player and show them in 3x4 grid\n",
        "plt.figure(figsize=(16, 7))\n",
        "for i, (player, tokens) in enumerate(tokens_by_player.items()):\n",
        "    wordcloud = WordCloud(width=400, height=175, background_color='white').generate(' '.join(tokens))\n",
        "    plt.subplot(3, 4, i+1)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(player)\n",
        "    plt.axis('off')\n",
        "\n",
        "#add summary word cloud to the end\n",
        "wordcloud = WordCloud(width=400, height=175, background_color='white').generate(' '.join(content_df['tokens'].sum()))\n",
        "plt.subplot(3, 4, 12)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Summary')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ## Sport Keywords Classifier -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# FOOTBALL_KEYWORDS = [\n",
        "#     'futebol', 'jogo', 'partida', 'pelada', 'clássico', 'gol', 'chute', 'passe', 'driblar', 'cabecear', 'chutar', 'defender', 'atacar', 'marcar', 'finalizar', 'cruzar', 'desarmar', 'interceptar', 'dominar', 'escanteio', 'falta', 'pênalti', 'impedimento', 'cartão', 'expulsão', 'substituição', 'pressão', 'contra-ataque', 'goleiro', 'zagueiro', 'lateral', 'volante', 'meia', 'atacante', 'centroavante', 'ponta', 'ala', 'zaga', 'time', 'equipe', 'seleção', 'clube', 'reserva', 'titular', 'jogador', 'técnico', 'treinador', 'árbitro', 'juiz', 'bandeirinha', 'artilheiro', 'capitão', 'campo', 'estádio', 'arena', 'gramado', 'vestiário', 'campeonato', 'copa', 'liga', 'mundial', 'libertadores', 'champions', 'brasileirão', 'série', 'divisão', 'torneio', 'vitória', 'derrota', 'empate', 'placar', 'resultado', 'classificação', 'eliminação', 'vencer', 'perder', 'empatar'\n",
        "# ]\n",
        "\n",
        "# class FootballClassifier:\n",
        "#     def __init__(self):\n",
        "#         self.nlp = spacy.load('pt_core_news_sm')\n",
        "#         self.football_keywords = FOOTBALL_KEYWORDS\n",
        "\n",
        "#     def preprocess_text(self, text):\n",
        "#         \"\"\"\n",
        "#         Preprocess the text by converting to lowercase and removing special characters\n",
        "#         \"\"\"\n",
        "#         text = text.lower()\n",
        "#         text = re.sub(r'[^a-záàâãéèêíïóôõöúçñ\\s]', '', text)\n",
        "#         return text\n",
        "\n",
        "#     def analyze_text(self, text, threshold=0.03):\n",
        "#         processed_text = self.preprocess_text(text)\n",
        "#         doc = self.nlp(processed_text)\n",
        "\n",
        "#         words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "#         football_words = [word for word in words if word in self.football_keywords]\n",
        "#         football_word_count = len(football_words)\n",
        "#         total_words = len(words)\n",
        "\n",
        "#         football_ratio = football_word_count / total_words if total_words > 0 else 0\n",
        "\n",
        "#         return {\n",
        "#             'is_football_related': football_ratio >= threshold,\n",
        "#             'confidence_score': football_ratio,\n",
        "#             'total_words': total_words,\n",
        "#             'football_words_count': football_word_count\n",
        "#         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connections Between Players by Shared Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes for players\n",
        "for player in content_df['Player'].unique():\n",
        "    G.add_node(player, type='player')\n",
        "\n",
        "all_words = content_df['tokens'].sum()\n",
        "most_common_words = [word for word, count in Counter(all_words).most_common(20)]\n",
        "\n",
        "# show node between players if they share a word\n",
        "for word in most_common_words:\n",
        "    players_with_word = content_df[content_df['tokens'].apply(lambda tokens: word in tokens)]['Player'].unique()\n",
        "    for i in range(len(players_with_word)):\n",
        "        for j in range(i + 1, len(players_with_word)):\n",
        "            G.add_edge(players_with_word[i], players_with_word[j], word=word)\n",
        "\n",
        "# Create the visualization with wider aspect ratio\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Set up the layout with adjusted parameters\n",
        "pos = nx.spring_layout(G,\n",
        "                      k=15,\n",
        "                      iterations=200,\n",
        "                      seed=126784)\n",
        "\n",
        "# Draw the network\n",
        "nx.draw_networkx_nodes(G, pos,\n",
        "                      node_size=100,\n",
        "                      node_color='lightblue',\n",
        "                      alpha=0.7)\n",
        "\n",
        "nx.draw_networkx_edges(G, pos,\n",
        "                      alpha=0.3,\n",
        "                      width=1.5)\n",
        "\n",
        "# Add labels\n",
        "labels = {node: node for node in G.nodes()}\n",
        "\n",
        "nx.draw_networkx_labels(G, pos,\n",
        "                       labels=labels,\n",
        "                       font_size=9,\n",
        "                       font_weight='bold')\n",
        "\n",
        "# Add word labels with adjusted position\n",
        "for word in most_common_words:\n",
        "    players_with_word = content_df[content_df['tokens'].apply(lambda tokens: word in tokens)]['Player'].unique()\n",
        "    if len(players_with_word) > 1:\n",
        "        x = sum(pos[player][0] for player in players_with_word) / len(players_with_word)\n",
        "        y = sum(pos[player][1] for player in players_with_word) / len(players_with_word)\n",
        "        plt.text(x, y, word,\n",
        "                fontsize=8,\n",
        "                ha='center',\n",
        "                va='center',\n",
        "                bbox=dict(facecolor='white',\n",
        "                         alpha=0.7,\n",
        "                         edgecolor='none',\n",
        "                         pad=1))\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network for Market Value Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create dataframe of player name, market value and tokens\n",
        "player_data = content_df.groupby('Player').agg({'tokens': 'sum'}).reset_index()\n",
        "\n",
        "# merge with market value data\n",
        "player_data = player_data.merge(merged_data[['Player', 'Market value']], on='Player', how='left')\n",
        "player_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| code-line-numbers: \"|6-10|11|12-19|44-61\"\n",
        "class PlayerValuePredictor:\n",
        "    def __init__(self):\n",
        "\n",
        "        # Create pipeline with Portuguese-specific TF-IDF\n",
        "        self.pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(\n",
        "                max_features=1000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2\n",
        "            )),\n",
        "            ('scaler', StandardScaler(with_mean=False)),\n",
        "            ('model', RandomForestRegressor(\n",
        "                n_estimators=500,\n",
        "                max_depth=None,\n",
        "                min_samples_split=3,\n",
        "                min_samples_leaf=2,\n",
        "                max_features='sqrt',\n",
        "                random_state=126784\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "    def preprocess_portuguese_text(self, text):\n",
        "        \"\"\"Preprocess Portuguese text\"\"\"\n",
        "        text = text.lower()\n",
        "        text = normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    def train(self, X_keywords, y_values, perform_grid_search=True):\n",
        "        \"\"\"Train the model with optional grid search\"\"\"\n",
        "        # Join list of keywords into space-separated strings if needed\n",
        "        if isinstance(X_keywords.iloc[0], list):\n",
        "            X_keywords = X_keywords.apply(' '.join)\n",
        "\n",
        "        # Log transform the target values if they're skewed\n",
        "        y_log = np.log1p(y_values)\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_keywords, y_log, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        if perform_grid_search:\n",
        "            # Define parameter grid\n",
        "            param_grid = {\n",
        "                'tfidf__max_features': [500, 1000, 2000],\n",
        "                'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "                'model__n_estimators': [100, 200],\n",
        "                'model__max_depth': [5, 10, 15],\n",
        "                'model__min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "\n",
        "            # Perform grid search\n",
        "            grid_search = GridSearchCV(\n",
        "                self.pipeline,\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            self.pipeline = grid_search.best_estimator_\n",
        "        else:\n",
        "            # Train with default parameters\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, keywords):\n",
        "        \"\"\"Predict market value for new keywords\"\"\"\n",
        "        # Handle both single string and list of keywords\n",
        "        if isinstance(keywords, list):\n",
        "            keywords = ' '.join(keywords)\n",
        "\n",
        "        # Convert to DataFrame series to match training format\n",
        "        keywords_series = pd.Series([keywords])\n",
        "\n",
        "        # Make prediction and transform back from log scale\n",
        "        predicted_value_log = self.pipeline.predict(keywords_series)\n",
        "        predicted_value = np.expm1(predicted_value_log)[0]\n",
        "\n",
        "        return predicted_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    predictor = PlayerValuePredictor()\n",
        "    predictor.train(player_data['tokens'], player_data['Market value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example\n",
        "\n",
        "![](tweet.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "text = 'Portugal se impuso este sábado por 3-0 a Turquía y se clasificó para los octavos de final de la Eurocopa 2024 como primera del Grupo F, gracias a un gol de Bernardo Silva, otro de Samet Akaydin en propia puerta y un tanto de Bruno Fernández'\n",
        "new_keywords = predictor.preprocess_portuguese_text(text)\n",
        "predicted_value = predictor.predict(new_keywords)\n",
        "print(f\"\\nPredicted value for new player: ${predicted_value:,.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        " - Advanced text processing & sentiment analysis\n",
        " - Visualizations of player connections\n",
        " - Market value trend prediction\n",
        "\n",
        "## Future Research Directions\n",
        "\n",
        " - Potential for real-time market value predictions\n",
        " - Expansion to other football leagues and languages\n",
        " - Integration with broader sports analytics systems\n",
        "\n",
        "## References\n",
        "\n",
        " - Sozen, Y. (2023). Predicting Football Players Market Value Using Machine Learning\n",
        " - Transfermarkt Documentation\n",
        " - Arquivo.pt API Documentation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/osolovei/Library/Python/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}